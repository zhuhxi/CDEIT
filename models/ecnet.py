import torch
import torch.nn as nn
import torch.nn.functional as F

# --- ç‰¹å¾èåˆæ¨¡å— ---
class FeatureFusion(nn.Module):
    def __init__(self):
        super(FeatureFusion, self).__init__()
        self.conv1x1 = nn.Conv2d(1, 16, kernel_size=1, padding=0)
        self.conv3x3 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv5x5 = nn.Conv2d(1, 48, kernel_size=5, padding=2)

    def forward(self, x):
        f1 = self.conv1x1(x)
        f2 = self.conv3x3(x)
        f3 = self.conv5x5(x)
        return torch.cat([f1, f2, f3], dim=1)  # è¾“å‡º (B, 96, H, W)

# --- å•ä¸ªè†¨èƒ€å·ç§¯Block ---
class DilatedConvBlock(nn.Module):
    def __init__(self):
        super(DilatedConvBlock, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(96, 96, kernel_size=3, padding=1, dilation=1),
            nn.BatchNorm2d(96),
            nn.ReLU(inplace=True)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(96, 96, kernel_size=3, padding=2, dilation=2),
            nn.BatchNorm2d(96),
            nn.ReLU(inplace=True)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(96, 96, kernel_size=3, padding=4, dilation=4),
            nn.BatchNorm2d(96),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        return x

# --- Ec-Net ä¸»ä½“ ---
class EcNet(nn.Module):
    def __init__(self):
        super(EcNet, self).__init__()
        self.upsample = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=False)  # ä»16x16åˆ°128x128
        self.feature_fusion = FeatureFusion()
        self.dilated_blocks = nn.Sequential(
            DilatedConvBlock(),
            DilatedConvBlock(),
            DilatedConvBlock(),
            DilatedConvBlock()
        )
        self.residual_reconstruct = nn.Conv2d(96, 1, kernel_size=3, padding=1)

    def forward(self, x):
        x = self.upsample(x)  # (B, 1, 128, 128)
        fused = self.feature_fusion(x)  # (B, 96, 128, 128)
        features = self.dilated_blocks(fused)  # (B, 96, 128, 128)

        # ğŸ” åŠ å…¥ long skip connectionï¼ˆç‰¹å¾èåˆæ¨¡å—è¾“å‡º + è†¨èƒ€å·ç§¯æ¨¡å—è¾“å‡ºï¼‰
        combined = features + fused

        residual = self.residual_reconstruct(combined)  # (B, 1, 128, 128)
        return residual  # è¾“å‡ºæ®‹å·®å›¾åƒï¼Œå¯åŠ å›åˆå§‹å›¾å¾—åˆ°æœ€ç»ˆå›¾åƒ


# æµ‹è¯•è„šæœ¬
if __name__ == '__main__':
    b = 1
    input_size = 16
    x = torch.rand(b, 1, input_size, input_size)  # è¾“å…¥: (B, 1, 16, 16)
    model = EcNet()

    # --- Shape æµ‹è¯• ---
    try:
        out = model(x)
        print(f"âœ… Forward Pass Success: {x.shape} â†’ {out.shape}")
    except Exception as e:
        print(f"âŒ Forward Failed: {e}")

    # --- FLOPs å’Œ å‚æ•°ç»Ÿè®¡ ---
    try:
        from nni.compression.utils.counter import count_flops_params
        flops, params, _ = count_flops_params(model, x=(x,))
        print(f"ğŸ“Š FLOPs: {flops / 1e6:.2f} MFLOPs | Params: {params / 1e6:.2f} M")
    except ImportError:
        print("âš ï¸ NNI not installed. Run: pip install nni")
