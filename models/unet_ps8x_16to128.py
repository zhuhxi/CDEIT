import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualConvBlock(nn.Module):
    """å¸¦æ®‹å·®è¿æ¥çš„æ·±åº¦å·ç§¯å—"""
    def __init__(self, in_ch, out_ch, dilation=1):
        super().__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=dilation, dilation=dilation),
            nn.BatchNorm2d(out_ch),
            nn.LeakyReLU(0.2, inplace=True))
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_ch, out_ch, 3, padding=dilation, dilation=dilation),
            nn.BatchNorm2d(out_ch),
            nn.LeakyReLU(0.2, inplace=True))
        self.shortcut = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()

    def forward(self, x):
        residual = self.shortcut(x)
        x = self.conv1(x)
        x = self.conv2(x)
        return x + residual

class DenseBlock(nn.Module):
    """å¯†é›†è¿æ¥å—å¢å¼ºç‰¹å¾å¤ç”¨"""
    def __init__(self, in_ch, growth_rate=32, num_layers=4):
        super().__init__()
        self.layers = nn.ModuleList()
        ch = in_ch
        for _ in range(num_layers):
            layer = nn.Sequential(
                nn.BatchNorm2d(ch),
                nn.LeakyReLU(0.2),
                nn.Conv2d(ch, growth_rate, 3, padding=1)
            )
            self.layers.append(layer)
            ch += growth_rate
        self.final_conv = nn.Conv2d(ch, in_ch, 1)  # ç‰¹å¾å‹ç¼©

    def forward(self, x):
        features = [x]
        for layer in self.layers:
            new_feat = layer(torch.cat(features, dim=1))
            features.append(new_feat)
        return self.final_conv(torch.cat(features, dim=1))

class DeepUNet_PS8x_16to128(nn.Module):
    def __init__(self):
        super().__init__()
        # ----------------- æ·±åº¦ç¼–ç å™¨ -----------------
        # è¾“å…¥: [B,1,16,16]
        self.enc1 = nn.Sequential(
            ResidualConvBlock(1, 64),
            DenseBlock(64, growth_rate=16, num_layers=3))
        self.pool1 = nn.MaxPool2d(2)  # [B,64,8,8]
        
        self.enc2 = nn.Sequential(
            ResidualConvBlock(64, 128, dilation=2),
            DenseBlock(128, growth_rate=32, num_layers=3))
        self.pool2 = nn.MaxPool2d(2)  # [B,128,4,4]
        
        self.enc3 = nn.Sequential(
            ResidualConvBlock(128, 256, dilation=2),
            ResidualConvBlock(256, 256),
            DenseBlock(256, growth_rate=32, num_layers=4)
        )
        self.pool3 = nn.MaxPool2d(2)  # [B,256,2,2]
        
        # ----------------- ç“¶é¢ˆå±‚ï¼ˆå¸¦ç©ºæ´å·ç§¯ï¼‰-----------------
        self.bottleneck = nn.Sequential(
            ResidualConvBlock(256, 512, dilation=3),
            ResidualConvBlock(512, 512),
            DenseBlock(512, growth_rate=64, num_layers=4),
            nn.Conv2d(512, 256, 1)  # é€šé“å‹ç¼©
        )  # [B,256,2,2]
        
        # ----------------- æ·±åº¦è§£ç å™¨ -----------------
        # ä¸Šé‡‡æ ·1: 2x2 -> 4x4
        self.up1 = nn.Sequential(
            nn.Conv2d(256, 256 * 4, 3, padding=1),
            nn.PixelShuffle(2),  # [B,256,4,4]
            nn.LeakyReLU(0.2)
        )
        self.dec1 = nn.Sequential(
            ResidualConvBlock(256 + 256, 256),  # è·³è·ƒè¿æ¥
            DenseBlock(256, growth_rate=32, num_layers=3)
        )
        
        # ä¸Šé‡‡æ ·2: 4x4 -> 8x8
        self.up2 = nn.Sequential(
            nn.Conv2d(256, 128 * 4, 3, padding=1),
            nn.PixelShuffle(2),  # [B,128,8,8]
            nn.LeakyReLU(0.2)
        )
        self.dec2 = nn.Sequential(
            ResidualConvBlock(128 + 128, 128),  # è·³è·ƒè¿æ¥
            DenseBlock(128, growth_rate=32, num_layers=2)
        )
        
        # ä¸Šé‡‡æ ·3: 8x8 -> 16x16
        self.up3 = nn.Sequential(
            nn.Conv2d(128, 64 * 4, 3, padding=1),
            nn.PixelShuffle(2),  # [B,64,16,16]
            nn.LeakyReLU(0.2)
        )
        self.dec3 = nn.Sequential(
            ResidualConvBlock(64 + 64, 64),  # è·³è·ƒè¿æ¥
            DenseBlock(64, growth_rate=16, num_layers=2)
        )
        
        # ----------------- æœ€ç»ˆä¸Šé‡‡æ · (16x16->128x128) -----------------
        self.final_up = nn.Sequential(
            nn.Conv2d(64, 64 * (8**2), 3, padding=1),  # 64*64=4096
            nn.PixelShuffle(8),  # [B,64,128,128]
            nn.LeakyReLU(0.2)
        )
        self.final_conv = nn.Sequential(
            ResidualConvBlock(64, 32),
            nn.Conv2d(32, 1, 1)  # è¾“å‡ºé€šé“
        )

    def forward(self, x):
        # ç¼–ç å™¨
        e1 = self.enc1(x)       # [B,64,16,16]
        p1 = self.pool1(e1)     # [B,64,8,8]
        e2 = self.enc2(p1)      # [B,128,8,8]
        p2 = self.pool2(e2)     # [B,128,4,4]
        e3 = self.enc3(p2)      # [B,256,4,4]
        p3 = self.pool3(e3)     # [B,256,2,2]
        
        # ç“¶é¢ˆå±‚
        b = self.bottleneck(p3) # [B,256,2,2]
        
        # è§£ç å™¨
        d1 = self.up1(b)        # [B,256,4,4]
        d1 = torch.cat([d1, e3], dim=1)  # [B,512,4,4]
        d1 = self.dec1(d1)      # [B,256,4,4]
        
        d2 = self.up2(d1)       # [B,128,8,8]
        d2 = torch.cat([d2, e2], dim=1)  # [B,256,8,8]
        d2 = self.dec2(d2)      # [B,128,8,8]
        
        d3 = self.up3(d2)       # [B,64,16,16]
        d3 = torch.cat([d3, e1], dim=1)  # [B,128,16,16]
        d3 = self.dec3(d3)      # [B,64,16,16]
        
        # æœ€ç»ˆä¸Šé‡‡æ ·
        out = self.final_up(d3) # [B,64,128,128]
        return self.final_conv(out)  # [B,1,128,128]

# ===================== å¢å¼ºæµ‹è¯•è„šæœ¬ =====================
if __name__ == '__main__':
    # é…ç½®æµ‹è¯•å‚æ•°
    batch_size = 4
    input_size = 16
    target_size = 128
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    model = DeepUNet_PS8x_16to128().to(device)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ€»é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ’ æ€»å‚æ•°é‡: {total_params/1e6:.2f}M")
    
    # æµ‹è¯•è¾“å…¥
    x = torch.rand(batch_size, 1, input_size, input_size).to(device)
    
    print("="*60)
    print(f"ğŸ” æµ‹è¯•æ·±åº¦æ¨¡å‹: {model.__class__.__name__}")
    print(f"ğŸ“¦ è¾“å…¥å°ºå¯¸: {x.shape} (batch={batch_size}, channels=1, {input_size}x{input_size})")
    
    # ç»´åº¦éªŒè¯
    try:
        out = model(x)
        expected_shape = (batch_size, 1, target_size, target_size)
        assert out.shape == expected_shape, \
            f"âŒ è¾“å‡ºå°ºå¯¸é”™è¯¯: é¢„æœŸ {expected_shape}, å®é™… {out.shape}"
        print(f"âœ… å‰å‘ä¼ æ’­éªŒè¯é€šè¿‡: {x.shape} â†’ {out.shape}")
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}")
        raise

    # è®¡ç®—é‡åˆ†æ
    try:
        from thop import profile
        flops, params = profile(model, inputs=(x,), verbose=False)
        print(f"ğŸ“Š FLOPs: {flops/1e9:.2f}G | å‚æ•°é‡: {params/1e6:.2f}M")
    except ImportError:
        print("âš ï¸ å®‰è£…thopä»¥è·å–è®¡ç®—é‡ç»Ÿè®¡: pip install thop")
    
    # å†…å­˜å ç”¨åˆ†æ
    try:
        mem_alloc = torch.cuda.memory_allocated(device) / 1024**2
        print(f"ğŸ’¾ GPUå†…å­˜å ç”¨: {mem_alloc:.2f} MB (å‰å‘ä¼ æ’­å)")
    except:
        pass
    
    # æ¢¯åº¦æ£€æŸ¥
    try:
        loss = F.l1_loss(out, torch.randn_like(out))
        loss.backward()
        print("âœ… æ¢¯åº¦åå‘ä¼ æ’­æˆåŠŸ")
        
        # æ£€æŸ¥æ¢¯åº¦çˆ†ç‚¸
        max_grad = max(p.grad.abs().max().item() for p in model.parameters() if p.grad is not None)
        print(f"ğŸ“ˆ æœ€å¤§æ¢¯åº¦å€¼: {max_grad:.4f}")
    except Exception as e:
        print(f"âŒ æ¢¯åº¦åå‘ä¼ æ’­å¤±è´¥: {str(e)}")
    
    # å…³é”®ç‰¹å¾å›¾å°ºå¯¸éªŒè¯
    print("\nğŸ”¬ æ·±åº¦ç‰¹å¾éªŒè¯:")
    with torch.no_grad():
        e1 = model.enc1(x)
        print(f"  enc1 è¾“å‡º: {e1.shape} (é¢„æœŸ [B,64,16,16])")
        e3 = model.enc3(model.pool2(model.enc2(model.pool1(e1))))
        print(f"  enc3 è¾“å‡º: {e3.shape} (é¢„æœŸ [B,256,4,4])")
        b = model.bottleneck(model.pool3(e3))
        print(f"  bottleneck è¾“å‡º: {b.shape} (é¢„æœŸ [B,256,2,2])")
        d1 = model.dec1(torch.cat([model.up1(b), e3], dim=1))
        print(f"  dec1 è¾“å‡º: {d1.shape} (é¢„æœŸ [B,256,4,4])")
        
    print("="*60)
