import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualConvBlock(nn.Module):
    """å¸¦æ¢¯åº¦ç¨³å®šæŠ€æœ¯çš„æ®‹å·®å—"""
    def __init__(self, in_ch, out_ch, dilation=1):
        super().__init__()
        # ä½¿ç”¨LayerNormæ›¿ä»£BatchNormä»¥è·å¾—æ›´ç¨³å®šçš„è®­ç»ƒ
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=dilation, dilation=dilation),
            nn.GroupNorm(4, out_ch),  # ä½¿ç”¨GroupNormæ›¿ä»£BatchNorm
            nn.ELU(inplace=True)  # ä½¿ç”¨ELUæ¿€æ´»å‡½æ•°æ›¿ä»£LeakyReLU
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_ch, out_ch, 3, padding=dilation, dilation=dilation),
            nn.GroupNorm(4, out_ch),
            nn.ELU(inplace=True))
        
        # æ·»åŠ æ¢¯åº¦ç¼©æ”¾å±‚
        self.grad_scale = nn.Sequential(
            nn.Conv2d(out_ch, out_ch, 1),
            nn.Sigmoid()
        )
        
        self.shortcut = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()

    def forward(self, x):
        residual = self.shortcut(x)
        x = self.conv1(x)
        x = self.conv2(x)
        
        # åº”ç”¨æ¢¯åº¦ç¼©æ”¾
        scale_factor = self.grad_scale(x)
        x = x * scale_factor
        
        return x + residual

class DenseBlock(nn.Module):
    """å¸¦æ¢¯åº¦æ§åˆ¶çš„å¯†é›†è¿æ¥å—"""
    def __init__(self, in_ch, growth_rate=32, num_layers=4):
        super().__init__()
        self.layers = nn.ModuleList()
        ch = in_ch
        
        for i in range(num_layers):
            layer = nn.Sequential(
                nn.GroupNorm(4, ch),  # ä½¿ç”¨GroupNorm
                nn.ELU(),  # ä½¿ç”¨ELUæ¿€æ´»å‡½æ•°
                nn.Conv2d(ch, growth_rate, 3, padding=1),
                
                # æ·»åŠ æ¢¯åº¦é—¨æ§
                nn.Sequential(
                    nn.Conv2d(growth_rate, growth_rate, 1),
                    nn.Sigmoid()
                )
            )
            self.layers.append(layer)
            ch += growth_rate
        
        # æœ€ç»ˆå·ç§¯å±‚æ·»åŠ æƒé‡å½’ä¸€åŒ–
        self.final_conv = nn.utils.weight_norm(
            nn.Conv2d(ch, in_ch, 1), name='weight'
        )

    def forward(self, x):
        features = [x]
        for layer in self.layers:
            new_feat = layer(torch.cat(features, dim=1))
            
            # åº”ç”¨æ¢¯åº¦é—¨æ§
            conv_out, gate = new_feat[:, :-1], new_feat[:, -1:]
            gated_feat = conv_out * gate
            
            features.append(gated_feat)
        
        return self.final_conv(torch.cat(features, dim=1))

class DeepUNet_PS8x_16to128(nn.Module):
    def __init__(self):
        super().__init__()
        # ----------------- æ·±åº¦ç¼–ç å™¨ -----------------
        self.enc1 = nn.Sequential(
            ResidualConvBlock(1, 64),
            DenseBlock(64, growth_rate=16, num_layers=3)
        )
        self.pool1 = nn.AvgPool2d(2)  # ä½¿ç”¨AvgPoolæ›¿ä»£MaxPool
        
        self.enc2 = nn.Sequential(
            ResidualConvBlock(64, 128, dilation=2),
            DenseBlock(128, growth_rate=32, num_layers=3)
        )
        self.pool2 = nn.AvgPool2d(2)
        
        self.enc3 = nn.Sequential(
            ResidualConvBlock(128, 256, dilation=2),
            ResidualConvBlock(256, 256),
            DenseBlock(256, growth_rate=32, num_layers=4)
        )
        self.pool3 = nn.AvgPool2d(2)
        
        # ----------------- ç“¶é¢ˆå±‚ -----------------
        self.bottleneck = nn.Sequential(
            ResidualConvBlock(256, 512, dilation=3),
            ResidualConvBlock(512, 512),
            DenseBlock(512, growth_rate=64, num_layers=4),
            nn.Conv2d(512, 256, 1)
        )
        
        # ----------------- æ·±åº¦è§£ç å™¨ -----------------
        # ä¸Šé‡‡æ ·1: 2x2 -> 4x4
        self.up1 = nn.Sequential(
            nn.utils.weight_norm(nn.Conv2d(256, 256 * 4, 3, padding=1)),
            nn.PixelShuffle(2),
            nn.ELU()
        )
        self.dec1 = nn.Sequential(
            ResidualConvBlock(256 + 256, 256),
            DenseBlock(256, growth_rate=32, num_layers=3)
        )
        
        # ä¸Šé‡‡æ ·2: 4x4 -> 8x8
        self.up2 = nn.Sequential(
            nn.utils.weight_norm(nn.Conv2d(256, 128 * 4, 3, padding=1)),
            nn.PixelShuffle(2),
            nn.ELU()
        )
        self.dec2 = nn.Sequential(
            ResidualConvBlock(128 + 128, 128),
            DenseBlock(128, growth_rate=32, num_layers=2)
        )
        
        # ä¸Šé‡‡æ ·3: 8x8 -> 16x16
        self.up3 = nn.Sequential(
            nn.utils.weight_norm(nn.Conv2d(128, 64 * 4, 3, padding=1)),
            nn.PixelShuffle(2),
            nn.ELU()
        )
        self.dec3 = nn.Sequential(
            ResidualConvBlock(64 + 64, 64),
            DenseBlock(64, growth_rate=16, num_layers=2)
        )
        
        # ----------------- æœ€ç»ˆä¸Šé‡‡æ · -----------------
        self.final_up = nn.Sequential(
            nn.utils.weight_norm(nn.Conv2d(64, 64 * (8**2), 3, padding=1)),
            nn.PixelShuffle(8),
            nn.ELU()
        )
        self.final_conv = nn.Sequential(
            ResidualConvBlock(64, 32),
            nn.Conv2d(32, 1, 1)
        )

    def forward(self, x):
        # ç¼–ç å™¨
        e1 = self.enc1(x)
        p1 = self.pool1(e1)
        e2 = self.enc2(p1)
        p2 = self.pool2(e2)
        e3 = self.enc3(p2)
        p3 = self.pool3(e3)
        
        # ç“¶é¢ˆå±‚
        b = self.bottleneck(p3)
        
        # è§£ç å™¨
        d1 = self.up1(b)
        d1 = torch.cat([d1, e3], dim=1)
        d1 = self.dec1(d1)
        
        d2 = self.up2(d1)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)
        
        d3 = self.up3(d2)
        d3 = torch.cat([d3, e1], dim=1)
        d3 = self.dec3(d3)
        
        # æœ€ç»ˆä¸Šé‡‡æ ·
        out = self.final_up(d3)
        return self.final_conv(out)

# ===================== å¢å¼ºæµ‹è¯•è„šæœ¬ =====================
if __name__ == '__main__':
    # é…ç½®æµ‹è¯•å‚æ•°
    batch_size = 4
    input_size = 16
    target_size = 128
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    model = DeepUNet_PS8x_16to128().to(device)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ€»é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ’ æ€»å‚æ•°é‡: {total_params/1e6:.2f}M")
    
    # æµ‹è¯•è¾“å…¥
    x = torch.rand(batch_size, 1, input_size, input_size).to(device)
    
    print("="*60)
    print(f"ğŸ” æµ‹è¯•æ·±åº¦æ¨¡å‹: {model.__class__.__name__}")
    print(f"ğŸ“¦ è¾“å…¥å°ºå¯¸: {x.shape} (batch={batch_size}, channels=1, {input_size}x{input_size})")
    
    # ç»´åº¦éªŒè¯
    try:
        out = model(x)
        expected_shape = (batch_size, 1, target_size, target_size)
        assert out.shape == expected_shape, \
            f"âŒ è¾“å‡ºå°ºå¯¸é”™è¯¯: é¢„æœŸ {expected_shape}, å®é™… {out.shape}"
        print(f"âœ… å‰å‘ä¼ æ’­éªŒè¯é€šè¿‡: {x.shape} â†’ {out.shape}")
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}")
        raise

    # è®¡ç®—é‡åˆ†æ
    try:
        from thop import profile
        flops, params = profile(model, inputs=(x,), verbose=False)
        print(f"ğŸ“Š FLOPs: {flops/1e9:.2f}G | å‚æ•°é‡: {params/1e6:.2f}M")
    except ImportError:
        print("âš ï¸ å®‰è£…thopä»¥è·å–è®¡ç®—é‡ç»Ÿè®¡: pip install thop")
    
    # æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯•
    print("\nğŸ”¬ æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯•:")
    try:
        # åˆ›å»ºä¼˜åŒ–å™¨å¹¶è®¾ç½®æ¢¯åº¦è£å‰ª
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        max_grad_norm = 1.0  # æ¢¯åº¦è£å‰ªé˜ˆå€¼
        for i in range(10):  # æ¨¡æ‹Ÿ10ä¸ªè®­ç»ƒæ­¥éª¤
            # å‰å‘ä¼ æ’­
            out = model(x)
            target = torch.randn_like(out)
            loss = F.l1_loss(out, target)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            total_norm = 0.0
            for p in model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** 0.5
            
            # åº”ç”¨æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            
            # å‚æ•°æ›´æ–°
            optimizer.step()
            
            # æ‰“å°æ¢¯åº¦ä¿¡æ¯
            clipped_norm = 0.0
            for p in model.parameters():
                if p.grad is not None:
                    clipped_norm += p.grad.data.norm(2).item() ** 2
            clipped_norm = clipped_norm ** 0.5
            
            print(f"æ­¥éª¤ {i+1}: æŸå¤±={loss.item():.4f} | "
                  f"æ¢¯åº¦èŒƒæ•°={total_norm:.2f} | "
                  f"è£å‰ªå={clipped_norm:.2f} | "
                  f"ç¼©æ”¾æ¯”={clipped_norm/max(1e-6, total_norm):.2f}")
        
        print("âœ… æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµ‹è¯•å¤±è´¥: {str(e)}")
    
    # å…³é”®ç‰¹å¾å›¾å°ºå¯¸éªŒè¯
    print("\nğŸ”¬ æ·±åº¦ç‰¹å¾éªŒè¯:")
    with torch.no_grad():
        e1 = model.enc1(x)
        print(f"  enc1 è¾“å‡º: {e1.shape} (é¢„æœŸ [B,64,16,16])")
        e3 = model.enc3(model.pool2(model.enc2(model.pool1(e1))))
        print(f"  enc3 è¾“å‡º: {e3.shape} (é¢„æœŸ [B,256,4,4])")
        b = model.bottleneck(model.pool3(e3))
        print(f"  bottleneck è¾“å‡º: {b.shape} (é¢„æœŸ [B,256,2,2])")
        d1 = model.dec1(torch.cat([model.up1(b), e3], dim=1))
        print(f"  dec1 è¾“å‡º: {d1.shape} (é¢„æœŸ [B,256,4,4])")
        
    print("="*60)
